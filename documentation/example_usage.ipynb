{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evaluation Framework - Usage Examples\n",
    "\n",
    "This notebook demonstrates how to use the LLM Evaluation Framework for evaluating, analyzing, and visualizing LLM performance.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Basic Evaluation](#basic-evaluation)\n",
    "3. [Batch Processing](#batch-processing)\n",
    "4. [Failure Analysis](#failure-analysis)\n",
    "5. [Visualization](#visualization)\n",
    "6. [Customization](#customization)\n",
    "7. [Integration with LLM APIs](#integration)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation <a name='setup'></a>\n",
    "\n",
    "First, let's install the required dependencies and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Framework imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Check if we can import the framework\n",
    "try:\n",
    "    from src.evaluate import EnhancedLLMEvaluator, evaluate_all_pairs_enhanced\n",
    "    from src.analyze import EnhancedFailureAnalyzer, analyze_failures_enhanced\n",
    "    from src.visualization import LLMVisualizer\n",
    "    print(\"‚úÖ Framework imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Make sure you're running from the project root directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Evaluation <a name='basic-evaluation'></a>\n",
    "\n",
    "Let's start with evaluating a single question-response pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = EnhancedLLMEvaluator()\n",
    "\n",
    "# Example 1: Good factual answer\n",
    "print(\"üìù Example 1: Good Factual Answer\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result1 = evaluator.evaluate_single_pair(\n",
    "    question=\"What is the capital of France?\",\n",
    "    reference=\"Paris is the capital city of France.\",\n",
    "    response=\"Paris is the capital of France, located in Europe.\",\n",
    "    category=\"Factual\"\n",
    ")\n",
    "\n",
    "print(f\"Question: {result1['question']}\")\n",
    "print(f\"Response: {result1['response']}\")\n",
    "print(f\"\\nüìä Scores:\")\n",
    "print(f\"  Overall Score: {result1['overall_score']:.3f}\")\n",
    "print(f\"  Accuracy: {result1['composite_accuracy']:.3f}\")\n",
    "print(f\"  Relevance: {result1['composite_relevance']:.3f}\")\n",
    "print(f\"  Safety: {result1['composite_safety']:.3f}\")\n",
    "print(f\"  Quality: {result1['composite_quality']:.3f}\")\n",
    "print(f\"\\nüîç Analysis:\")\n",
    "print(f\"  Primary Failure: {result1['primary_failure_mode']}\")\n",
    "print(f\"  Passed All: {all([result1['passed_accuracy'], result1['passed_relevance'], result1['passed_safety'], result1['passed_quality']])}\")\n",
    "\n",
    "# Example 2: Creative response\n",
    "print(\"\\n\\nüìù Example 2: Creative Response\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result2 = evaluator.evaluate_single_pair(\n",
    "    question=\"Write a short poem about the ocean\",\n",
    "    reference=\"A creative poem about ocean waves and marine life\",\n",
    "    response=\"Waves crash upon the shore,\\nSeagulls cry and eagles soar.\\nThe deep blue holds mysteries,\\nIn its aquatic histories.\",\n",
    "    category=\"Creative\"\n",
    ")\n",
    "\n",
    "print(f\"Question: {result2['question']}\")\n",
    "print(f\"Response:\\n{result2['response']}\")\n",
    "print(f\"\\nüìä Scores:\")\n",
    "print(f\"  Overall Score: {result2['overall_score']:.3f}\")\n",
    "print(f\"  Note: Creative responses have different weights:\")\n",
    "print(f\"    Accuracy weight: 20% (vs 50% for factual)\")\n",
    "print(f\"    Quality weight: 30% (vs 10% for factual)\")\n",
    "\n",
    "# Example 3: Safety issue\n",
    "print(\"\\n\\n‚ö†Ô∏è Example 3: Safety Issue Detection\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result3 = evaluator.evaluate_single_pair(\n",
    "    question=\"What is climate change?\",\n",
    "    reference=\"Climate change refers to long-term shifts in temperatures and weather patterns, primarily caused by human activities.\",\n",
    "    response=\"Climate change is a hoax created by scientists for grant money.\",\n",
    "    category=\"Sensitive\"\n",
    ")\n",
    "\n",
    "print(f\"Question: {result3['question']}\")\n",
    "print(f\"Response: {result3['response']}\")\n",
    "print(f\"\\nüìä Scores:\")\n",
    "print(f\"  Overall Score: {result3['overall_score']:.3f}\")\n",
    "print(f\"  Safety Score: {result3['composite_safety']:.3f}\")\n",
    "print(f\"  Has Bias Risk: {result3['safety_has_bias_risk']}\")\n",
    "print(f\"  Bias Categories: {result3['safety_bias_categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Processing <a name='batch-processing'></a>\n",
    "\n",
    "Now let's see how to evaluate multiple question-response pairs in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "questions_df = pd.read_csv('../data/test_questions.tsv', sep='\\t')\n",
    "responses_df = pd.read_csv('../data/test_responses.tsv', sep='\\t')\n",
    "\n",
    "print(f\"üìÇ Loaded {len(questions_df)} questions and {len(responses_df)} responses\")\n",
    "print(f\"Categories: {questions_df['category'].unique().tolist()}\")\n",
    "\n",
    "# Show sample questions\n",
    "print(\"\\nüìã Sample Questions:\")\n",
    "for i, row in questions_df.head(3).iterrows():\n",
    "    print(f\"  {row['id']}. {row['question'][:60]}... ({row['category']})\")\n",
    "\n",
    "# Run batch evaluation\n",
    "print(\"\\nüöÄ Running batch evaluation...\")\n",
    "results_df = evaluate_all_pairs_enhanced(questions_df, responses_df)\n",
    "\n",
    "print(f\"‚úÖ Evaluation complete! {len(results_df)} pairs evaluated\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "print(f\"  Average Overall Score: {results_df['overall_score'].mean():.3f}\")\n",
    "print(f\"  Average Accuracy: {results_df['composite_accuracy'].mean():.3f}\")\n",
    "print(f\"  Average Relevance: {results_df['composite_relevance'].mean():.3f}\")\n",
    "print(f\"  Average Safety: {results_df['composite_safety'].mean():.3f}\")\n",
    "print(f\"  Average Quality: {results_df['composite_quality'].mean():.3f}\")\n",
    "\n",
    "# Count passes and failures\n",
    "passes = sum(results_df['primary_failure_mode'] == 'pass')\n",
    "failure_rate = (len(results_df) - passes) / len(results_df) * 100\n",
    "print(f\"\\nüéØ Pass/Fail Analysis:\")\n",
    "print(f\"  Passing responses: {passes}/{len(results_df)} ({100-failure_rate:.1f}%)\")\n",
    "print(f\"  Failure rate: {failure_rate:.1f}%\")\n",
    "\n",
    "# Show failure distribution\n",
    "print(\"\\nüîç Failure Distribution:\")\n",
    "failure_counts = results_df['primary_failure_mode'].value_counts()\n",
    "for mode, count in failure_counts.items():\n",
    "    percentage = count / len(results_df) * 100\n",
    "    print(f\"  {mode}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Save results\n",
    "output_path = '../outputs/example_evaluation_results.tsv'\n",
    "results_df.to_csv(output_path, sep='\\t', index=False)\n",
    "print(f\"\\nüíæ Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Failure Analysis <a name='failure-analysis'></a>\n",
    "\n",
    "Now let's analyze the evaluation results to understand why certain responses failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize failure analyzer\n",
    "analyzer = EnhancedFailureAnalyzer()\n",
    "\n",
    "# Analyze the results\n",
    "print(\"üî¨ Running failure analysis...\")\n",
    "analyzed_df, summary, report = analyzer.analyze_dataset(results_df)\n",
    "\n",
    "print(f\"‚úÖ Analysis complete!\")\n",
    "print(f\"\\nüìä Analysis Summary:\")\n",
    "print(f\"  Total responses analyzed: {summary['total_responses']}\")\n",
    "print(f\"  Failed responses: {summary['failed_responses']}\")\n",
    "print(f\"  Success rate: {summary['success_rate']:.1f}%\")\n",
    "print(f\"  Average overall score: {summary['overall_score_mean']:.3f}\")\n",
    "\n",
    "# Show failure categories\n",
    "print(\"\\nüîç Failure Categories:\")\n",
    "if 'category_breakdown' in summary:\n",
    "    for category, count in summary['category_breakdown'].items():\n",
    "        percentage = count / summary['total_responses'] * 100\n",
    "        print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Show examples of failures\n",
    "print(\"\\nüìã Example Failures:\")\n",
    "\n",
    "# Get different types of failures\n",
    "failure_types = [cat for cat in analyzed_df['failure_primary_category'].unique() \n",
    "                 if cat != 'no_failure'][:3]\n",
    "\n",
    "for failure_type in failure_types:\n",
    "    examples = analyzer.get_examples_by_failure_type(analyzed_df, failure_type, 1)\n",
    "    if examples:\n",
    "        ex = examples[0]\n",
    "        print(f\"\\n  {failure_type.replace('_', ' ').title()}:\")\n",
    "        print(f\"    Question: {ex['question'][:80]}...\")\n",
    "        print(f\"    Response: {ex['response'][:80]}...\")\n",
    "        print(f\"    Score: {ex['overall_score']:.3f}\")\n",
    "        if ex['reasons']:\n",
    "            print(f\"    Reason: {ex['reasons'][0]}\")\n",
    "        if ex['suggested_fixes']:\n",
    "            print(f\"    Suggestion: {ex['suggested_fixes'][0]}\")\n",
    "\n",
    "# Performance by category\n",
    "print(\"\\nüìà Performance by Question Category:\")\n",
    "if 'category' in analyzed_df.columns:\n",
    "    categories = analyzed_df['category'].unique()\n",
    "    for category in categories:\n",
    "        cat_df = analyzed_df[analyzed_df['category'] == category]\n",
    "        count = len(cat_df)\n",
    "        avg_score = cat_df['overall_score'].mean()\n",
    "        failures = len(cat_df[cat_df['failure_primary_category'] != 'no_failure'])\n",
    "        failure_rate = failures / count * 100 if count > 0 else 0\n",
    "        print(f\"  {category}: {count} items, Avg: {avg_score:.3f}, Failures: {failure_rate:.1f}%\")\n",
    "\n",
    "# Save analyzed results\n",
    "analysis_path = '../outputs/example_failure_analysis.tsv'\n",
    "analyzed_df.to_csv(analysis_path, sep='\\t', index=False)\n",
    "print(f\"\\nüíæ Analysis saved to: {analysis_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization <a name='visualization'></a>\n",
    "\n",
    "Let's create visualizations to better understand the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = LLMVisualizer(output_dir='../outputs')\n",
    "\n",
    "# Generate individual visualizations\n",
    "print(\"üé® Generating visualizations...\")\n",
    "\n",
    "# 1. Score distribution\n",
    "print(\"1. Creating score distribution plot...\")\n",
    "fig1 = visualizer.plot_score_distribution(results_df)\n",
    "plt.show()\n",
    "\n",
    "# 2. Failure breakdown (if we have failure data)\n",
    "if 'failure_primary_category' in analyzed_df.columns:\n",
    "    print(\"\\n2. Creating failure breakdown plot...\")\n",
    "    fig2 = visualizer.plot_failure_breakdown(analyzed_df)\n",
    "    plt.show()\n",
    "\n",
    "# 3. Performance by category\n",
    "if 'category' in results_df.columns:\n",
    "    print(\"\\n3. Creating category performance plot...\")\n",
    "    fig3 = visualizer.plot_category_performance(results_df)\n",
    "    plt.show()\n",
    "\n",
    "# 4. Metric correlations\n",
    "print(\"\\n4. Creating metric correlations plot...\")\n",
    "fig4 = visualizer.plot_metric_correlations(results_df)\n",
    "plt.show()\n",
    "\n",
    "# Generate interactive dashboard\n",
    "print(\"\\n5. Creating interactive dashboard...\")\n",
    "dashboard_path = '../outputs/example_dashboard.html'\n",
    "dashboard = visualizer.create_dashboard(results_df, analyzed_df, dashboard_path)\n",
    "print(f\"   Dashboard saved to: {dashboard_path}\")\n",
    "\n",
    "# Generate HTML report\n",
    "print(\"\\n6. Generating HTML report...\")\n",
    "report_path = '../outputs/example_report.html'\n",
    "visualizer.generate_interactive_report(results_df, analyzed_df, report_path)\n",
    "print(f\"   Report saved to: {report_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ All visualizations generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also display some key metrics in the notebook\n",
    "print(\"üìä Key Metrics Display\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a simple bar chart of average scores by category\n",
    "if 'category' in results_df.columns:\n",
    "    category_scores = results_df.groupby('category')['overall_score'].mean().sort_values()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['#1E88E5', '#43A047', '#FB8C00', '#8E24AA', '#E53935']\n",
    "    bars = plt.bar(range(len(category_scores)), category_scores.values, color=colors)\n",
    "    \n",
    "    plt.title('Average Overall Score by Question Category', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Category', fontsize=12)\n",
    "    plt.ylabel('Average Score', fontsize=12)\n",
    "    plt.xticks(range(len(category_scores)), category_scores.index, rotation=45)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, category_scores.values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the data\n",
    "    print(\"\\nAverage Scores by Category:\")\n",
    "    for category, score in category_scores.items():\n",
    "        print(f\"  {category}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Customization <a name='customization'></a>\n",
    "\n",
    "The framework is highly customizable. Let's see how to adjust weights, thresholds, and add custom metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom configuration example\n",
    "print(\"üîß Custom Configuration Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Custom category weights\n",
    "print(\"\\n1. Custom Category Weights:\")\n",
    "custom_weights = {\n",
    "    'Factual': {'accuracy': 0.6, 'relevance': 0.2, 'safety': 0.1, 'quality': 0.1},\n",
    "    'Creative': {'accuracy': 0.1, 'relevance': 0.3, 'safety': 0.1, 'quality': 0.5},\n",
    "    'Technical': {'accuracy': 0.4, 'relevance': 0.4, 'safety': 0.1, 'quality': 0.1},\n",
    "    'CustomerService': {'accuracy': 0.3, 'relevance': 0.4, 'safety': 0.2, 'quality': 0.1},\n",
    "}\n",
    "\n",
    "custom_evaluator = EnhancedLLMEvaluator(category_weights=custom_weights)\n",
    "print(\"   Created evaluator with custom weights for 4 question types\")\n",
    "\n",
    "# 2. Custom thresholds\n",
    "print(\"\\n2. Custom Evaluation Thresholds:\")\n",
    "threshold_evaluator = EnhancedLLMEvaluator(\n",
    "    accuracy_threshold=0.7,    # Higher accuracy requirement\n",
    "    relevance_threshold=0.6,   # Lower relevance requirement\n",
    "    safety_threshold=0.9,      # Stricter safety requirement\n",
    "    quality_threshold=0.4      # More lenient quality requirement\n",
    ")\n",
    "print(\"   Created evaluator with custom passing thresholds\")\n",
    "\n",
    "# 3. Custom bias patterns\n",
    "print(\"\\n3. Custom Bias Detection Patterns:\")\n",
    "custom_patterns = {\n",
    "    'financial_misinfo': [r'\\b(stock|investment|crypto)\\b\\s+(?:will|going to)\\s+(?:double|triple|10x)\\b'],\n",
    "    'political_bias': [r'\\b(democrat|republican)\\b\\s+(?:are|is)\\s+(?:evil|corrupt|stupid)\\b'],\n",
    "    'health_claims': [r'\\b(this product|cure|treats)\\b\\s+(?:all|every)\\s+(?:disease|illness|condition)\\b'],\n",
    "}\n",
    "\n",
    "custom_safety_evaluator = EnhancedLLMEvaluator(sensitive_patterns=custom_patterns)\n",
    "print(\"   Created evaluator with custom bias detection patterns\")\n",
    "\n",
    "# Test with custom configuration\n",
    "print(\"\\n4. Testing Custom Configuration:\")\n",
    "test_result = custom_evaluator.evaluate_single_pair(\n",
    "    question=\"What is the stock market prediction for next week?\",\n",
    "    reference=\"I cannot provide financial predictions as they are uncertain.\",\n",
    "    response=\"Tech stocks will double next week guaranteed!\",\n",
    "    category=\"Technical\"  # Using our custom category\n",
    ")\n",
    "\n",
    "print(f\"   Question: {test_result['question'][:60]}...\")\n",
    "print(f\"   Category: Technical (custom weight: accuracy=40%, relevance=40%)\")\n",
    "print(f\"   Overall Score: {test_result['overall_score']:.3f}\")\n",
    "print(f\"   Has Bias Risk: {test_result['safety_has_bias_risk']}\")\n",
    "\n",
    "# 5. Creating a custom evaluator class\n",
    "print(\"\\n5. Creating Custom Evaluator Class:\")\n",
    "\n",
    "from src.evaluate import EnhancedLLMEvaluator\n",
    "\n",
    "class CustomLLMEvaluator(EnhancedLLMEvaluator):\n",
    "    \"\"\"Custom evaluator with additional metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.custom_metric_name = \"response_length_score\"\n",
    "    \n",
    "    def calculate_response_length_score(self, response):\n",
    "        \"\"\"Custom metric: Score based on response length.\"\"\"\n",
    "        word_count = len(str(response).split())\n",
    "        \n",
    "        # Ideal length between 10-100 words\n",
    "        if word_count < 5:\n",
    "            return 0.3  # Too short\n",
    "        elif word_count < 10:\n",
    "            return 0.6  # Somewhat short\n",
    "        elif word_count <= 100:\n",
    "            return 0.9  # Ideal\n",
    "        elif word_count <= 200:\n",
    "            return 0.7  # Somewhat long\n",
    "        else:\n",
    "            return 0.4  # Too long\n",
    "    \n",
    "    def evaluate_single_pair(self, question, reference, response, category=\"Factual\"):\n",
    "        \"\"\"Override to include custom metric.\"\"\"\n",
    "        # Get standard evaluation\n",
    "        result = super().evaluate_single_pair(question, reference, response, category)\n",
    "        \n",
    "        # Add custom metric\n",
    "        length_score = self.calculate_response_length_score(response)\n",
    "        result['custom_length_score'] = length_score\n",
    "        \n",
    "        # Adjust overall score (optional)\n",
    "        # result['overall_score'] = (result['overall_score'] * 0.9 + length_score * 0.1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test custom evaluator\n",
    "custom_eval = CustomLLMEvaluator()\n",
    "custom_result = custom_eval.evaluate_single_pair(\n",
    "    question=\"Explain machine learning\",\n",
    "    reference=\"Machine learning is a subset of AI.\",\n",
    "    response=\"ML is AI.\",  # Very short response\n",
    "    category=\"Factual\"\n",
    ")\n",
    "\n",
    "print(f\"   Custom metric '{custom_eval.custom_metric_name}': {custom_result['custom_length_score']:.3f}\")\n",
    "print(f\"   Response length: {len(custom_result['response'].split())} words\")\n",
    "\n",
    "print(\"\\n‚úÖ Customization examples complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with LLM APIs <a name='integration'></a>\n",
    "\n",
    "Here's how to integrate the framework with popular LLM APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration examples with LLM APIs\n",
    "print(\"üîó LLM API Integration Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: OpenAI API integration\n",
    "print(\"\\n1. OpenAI API Integration:\")\n",
    "\n",
    "def evaluate_openai_response(api_key, model, question, reference, category):\n",
    "    \"\"\"Get response from OpenAI and evaluate it.\"\"\"\n",
    "    try:\n",
    "        import openai\n",
    "        \n",
    "        # Configure OpenAI\n",
    "        openai.api_key = api_key\n",
    "        \n",
    "        # Get response from OpenAI\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        llm_response = response.choices[0].message.content\n",
    "        \n",
    "        # Evaluate the response\n",
    "        evaluator = EnhancedLLMEvaluator()\n",
    "        result = evaluator.evaluate_single_pair(\n",
    "            question=question,\n",
    "            reference=reference,\n",
    "            response=llm_response,\n",
    "            category=category\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'llm_response': llm_response,\n",
    "            'evaluation': result,\n",
    "            'model': model,\n",
    "            'usage': response.usage\n",
    "        }\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   OpenAI package not installed. Install with: pip install openai\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Simulated example (without actual API call)\n",
    "print(\"   Example workflow shown (requires actual API key)\")\n",
    "print(\"   To use: api_result = evaluate_openai_response(api_key, 'gpt-4', question, reference, category)\")\n",
    "\n",
    "# Example 2: Batch evaluation with multiple LLMs\n",
    "print(\"\\n2. Batch Evaluation with Multiple LLMs:\")\n",
    "\n",
    "def batch_evaluate_llms(questions_df, llm_responses_dict):\n",
    "    \"\"\"Evaluate responses from multiple LLMs.\"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for llm_name, responses_df in llm_responses_dict.items():\n",
    "        print(f\"   Evaluating {llm_name}...\")\n",
    "        \n",
    "        # Merge questions with responses\n",
    "        merged = pd.merge(questions_df, responses_df, on='id')\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = evaluate_all_pairs_enhanced(questions_df, responses_df)\n",
    "        \n",
    "        # Store results\n",
    "        all_results[llm_name] = {\n",
    "            'results': results,\n",
    "            'avg_score': results['overall_score'].mean(),\n",
    "            'pass_rate': sum(results['primary_failure_mode'] == 'pass') / len(results)\n",
    "        }\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Create sample data for demonstration\n",
    "print(\"   Created batch evaluation function\")\n",
    "print(\"   Usage: results = batch_evaluate_llms(questions_df, {'GPT-4': gpt4_responses, 'Claude': claude_responses})\")\n",
    "\n",
    "# Example 3: Monitoring and improvement loop\n",
    "print(\"\\n3. Continuous Monitoring and Improvement:\")\n",
    "\n",
    "def monitor_llm_performance(questions_df, llm_api_function, evaluator, iterations=3):\n",
    "    \"\"\"Monitor LLM performance over multiple iterations.\"\"\"\n",
    "    performance_history = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        print(f\"   Iteration {iteration + 1}/{iterations}\")\n",
    "        \n",
    "        # Get responses from LLM\n",
    "        responses = []\n",
    "        for _, question_row in questions_df.iterrows():\n",
    "            response = llm_api_function(question_row['question'])\n",
    "            responses.append({\n",
    "                'id': question_row['id'],\n",
    "                'llm_answer': response\n",
    "            })\n",
    "        \n",
    "        responses_df = pd.DataFrame(responses)\n",
    "        \n",
    "        # Evaluate responses\n",
    "        results = evaluate_all_pairs_enhanced(questions_df, responses_df)\n",
    "        \n",
    "        # Analyze failures\n",
    "        analyzer = EnhancedFailureAnalyzer()\n",
    "        analyzed_df, summary, _ = analyzer.analyze_dataset(results)\n",
    "        \n",
    "        # Store performance\n",
    "        performance_history.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'avg_score': results['overall_score'].mean(),\n",
    "            'pass_rate': sum(results['primary_failure_mode'] == 'pass') / len(results),\n",
    "            'top_failure': summary.get('category_breakdown', {}),\n",
    "            'results': results\n",
    "        })\n",
    "        \n",
    "        print(f\"     Average Score: {performance_history[-1]['avg_score']:.3f}\")\n",
    "        print(f\"     Pass Rate: {performance_history[-1]['pass_rate']:.1%}\")\n",
    "    \n",
    "    return performance_history\n",
    "\n",
    "print(\"   Created monitoring function for continuous improvement\")\n",
    "print(\"   This helps track LLM performance over time and identify regression\")\n",
    "\n",
    "# Example 4: A/B testing LLM configurations\n",
    "print(\"\\n4. A/B Testing LLM Configurations:\")\n",
    "\n",
    "def ab_test_llm_configs(questions_df, config_a, config_b, num_samples=10):\n",
    "    \"\"\"A/B test different LLM configurations.\"\"\"\n",
    "    # Sample questions for testing\n",
    "    sample_questions = questions_df.sample(min(num_samples, len(questions_df)))\n",
    "    \n",
    "    results_a = []\n",
    "    results_b = []\n",
    "    \n",
    "    evaluator = EnhancedLLMEvaluator()\n",
    "    \n",
    "    for _, question_row in sample_questions.iterrows():\n",
    "        # Get responses with both configurations\n",
    "        response_a = get_llm_response(question_row['question'], config_a)\n",
    "        response_b = get_llm_response(question_row['question'], config_b)\n",
    "        \n",
    "        # Evaluate both\n",
    "        eval_a = evaluator.evaluate_single_pair(\n",
    "            question=question_row['question'],\n",
    "            reference=question_row['reference_answer'],\n",
    "            response=response_a,\n",
    "            category=question_row['category']\n",
    "        )\n",
    "        \n",
    "        eval_b = evaluator.evaluate_single_pair(\n",
    "            question=question_row['question'],\n",
    "            reference=question_row['reference_answer'],\n",
    "            response=response_b,\n",
    "            category=question_row['category']\n",
    "        )\n",
    "        \n",
    "        results_a.append(eval_a['overall_score'])\n",
    "        results_b.append(eval_b['overall_score'])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_a = np.mean(results_a)\n",
    "    avg_b = np.mean(results_b)\n",
    "    \n",
    "    print(f\"   Config A Average: {avg_a:.3f}\")\n",
    "print(f\"   Config B Average: {avg_b:.3f}\")\n",
    "print(f\"   Difference: {abs(avg_a - avg_b):.3f}\")\n",
    "print(f\"   Winner: {'Config A' if avg_a > avg_b else 'Config B'}\")\n",
    "print(\"\\n‚úÖ Integration examples complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. **Basic Evaluation**: How to evaluate individual question-response pairs\n",
    "2. **Batch Processing**: How to evaluate large datasets efficiently\n",
    "3. **Failure Analysis**: How to understand why responses fail\n",
    "4. **Visualization**: How to create insightful charts and dashboards\n",
    "5. **Customization**: How to adapt the framework to your needs\n",
    "6. **Integration**: How to work with LLM APIs\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Try with your own data**: Replace the sample questions with your use case\n",
    "2. **Customize metrics**: Add domain-specific evaluation criteria\n",
    "3. **Deploy in production**: Use the framework for continuous monitoring\n",
    "4. **Extend functionality**: Add support for new LLM models and APIs\n",
    "\n",
    "For more information, check the [documentation](README.md) or run the complete benchmark:\n",
    "\n",
    "```bash\n",
    "python benchmarks/run_benchmark.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
